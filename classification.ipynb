{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Downloads/ECS192/Datasets/Replication_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Replication</th>\n",
       "      <th>Score_replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it will no influence DNA replication</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The G to A base change will not affect replica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During DNA replication the entire DNA will be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In DNA replication the DNA would simply replic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It wont have any direct effect besides that th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>It will stop the function of the DNA.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>It will stop replication early.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>It will stop earlier then it should.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>The DNA will stop replication at the stop codo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>It will likely not proceed.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1031 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Replication  Score_replication\n",
       "0                  it will no influence DNA replication                  1\n",
       "1     The G to A base change will not affect replica...                  1\n",
       "2     During DNA replication the entire DNA will be ...                  1\n",
       "3     In DNA replication the DNA would simply replic...                  1\n",
       "4     It wont have any direct effect besides that th...                  1\n",
       "...                                                 ...                ...\n",
       "1026              It will stop the function of the DNA.                  3\n",
       "1027                    It will stop replication early.                  3\n",
       "1028               It will stop earlier then it should.                  3\n",
       "1029  The DNA will stop replication at the stop codo...                  3\n",
       "1030                        It will likely not proceed.                  3\n",
       "\n",
       "[1031 rows x 2 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Replication']\n",
    "y = df['Score_replication']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# y = LabelBinarizer().fit_transform(df.Score_replication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# mylist = [0,0,0]\n",
    "# df['one hot'] = np.tile(mylist, (len(df),1)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(df)):\n",
    "#     df['one hot'].iloc[i] = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the total number of words\n",
    "df['new_column'] = df.Replication.apply(lambda x: len(str(x).split(' ')))\n",
    "num_words = df.new_column.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = X.tolist()\n",
    "labels = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(num_words, sequence_length, test_size=0.2, oov_token=None):\n",
    "    # tokenize the dataset corpus, delete uncommon words such as names, etc.\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    X = tokenizer.texts_to_sequences(text)\n",
    "    X, y = np.array(X), np.array(labels)\n",
    "    # pad sequences with 0's\n",
    "    X = pad_sequences(X, maxlen=sequence_length)\n",
    "    # convert labels to one-hot encoded\n",
    "    y = to_categorical(y)\n",
    "    # split data to training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    data = {}\n",
    "    data[\"X_train\"] = X_train\n",
    "    data[\"X_test\"]= X_test\n",
    "    data[\"y_train\"] = y_train\n",
    "    data[\"y_test\"] = y_test\n",
    "    data[\"tokenizer\"] = tokenizer\n",
    "    data[\"int2label\"] =  {0: \"1\", 1: \"2\", 2: \"3\"}\n",
    "    data[\"label2int\"] = {\"1\": 0, \"2\": 1, \"3\": 2}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_vectors(word_index, embedding_size=100):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n",
    "    with open(f\"../Downloads/glove.42B.300d.txt\", encoding=\"utf8\") as f:\n",
    "        for line in tqdm(f, \"Reading GloVe\"):\n",
    "            values = line.split()\n",
    "            # get the word as the first word in the line\n",
    "            word = values[0]\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                # get the vectors as the remaining values in the line\n",
    "                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(word_index, units=128, n_layers=1, cell=LSTM, bidirectional=False,\n",
    "                embedding_size=100, sequence_length=100, dropout=0.3, \n",
    "                loss=\"categorical_crossentropy\", optimizer=\"adam\", \n",
    "                output_length=2):\n",
    "    \"\"\"Constructs a RNN model given its parameters\"\"\"\n",
    "    embedding_matrix = get_embedding_vectors(word_index, embedding_size)\n",
    "    model = Sequential()\n",
    "    # add the embedding layer\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "              embedding_size,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              input_length=sequence_length))\n",
    "    for i in range(n_layers):\n",
    "        if i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # first layer or hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(output_length, activation=\"softmax\"))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max number of words in each sentence\n",
    "SEQUENCE_LENGTH = 300\n",
    "# N-Dimensional GloVe embedding vectors\n",
    "EMBEDDING_SIZE = 300\n",
    "# number of words to use, discarding the rest\n",
    "N_WORDS = 10000\n",
    "# out of vocabulary token\n",
    "OOV_TOKEN = None\n",
    "# 30% testing set, 70% training set\n",
    "TEST_SIZE = 0.2\n",
    "# number of CELL layers\n",
    "N_LAYERS = 1\n",
    "# the RNN cell to use, LSTM in this case\n",
    "RNN_CELL = LSTM\n",
    "# whether it's a bidirectional RNN\n",
    "IS_BIDIRECTIONAL = False\n",
    "# number of units (RNN_CELL ,nodes) in each layer\n",
    "UNITS = 128\n",
    "# dropout rate\n",
    "DROPOUT = 0.4\n",
    "### Training parameters\n",
    "LOSS = \"categorical_crossentropy\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "\n",
    "def get_model_name(dataset_name):\n",
    "    # construct the unique model name\n",
    "    model_name = f\"{dataset_name}-{RNN_CELL.__name__}-seq-{SEQUENCE_LENGTH}-em-{EMBEDDING_SIZE}-w-{N_WORDS}-layers-{N_LAYERS}-units-{UNITS}-opt-{OPTIMIZER}-BS-{BATCH_SIZE}-d-{DROPOUT}\"\n",
    "    if IS_BIDIRECTIONAL:\n",
    "        # add 'bid' str if bidirectional\n",
    "        model_name = \"bid-\" + model_name\n",
    "    if OOV_TOKEN:\n",
    "        # add 'oov' str if OOV token is specified\n",
    "        model_name += \"-oov\"\n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-7db7c6a0cf1e>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  X, y = np.array(X), np.array(labels)\n",
      "Reading GloVe: 1917495it [00:50, 38281.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          346800    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 566,964\n",
      "Trainable params: 220,164\n",
      "Non-trainable params: 346,800\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "13/13 [==============================] - 8s 498ms/step - loss: 1.0742 - accuracy: 0.4927 - val_loss: 0.9242 - val_accuracy: 0.5942\n",
      "Epoch 2/10\n",
      "13/13 [==============================] - 6s 497ms/step - loss: 0.8067 - accuracy: 0.6359 - val_loss: 0.7454 - val_accuracy: 0.7053\n",
      "Epoch 3/10\n",
      "13/13 [==============================] - 7s 527ms/step - loss: 0.6664 - accuracy: 0.7245 - val_loss: 0.7084 - val_accuracy: 0.7005\n",
      "Epoch 4/10\n",
      "13/13 [==============================] - 6s 483ms/step - loss: 0.6065 - accuracy: 0.7633 - val_loss: 0.6457 - val_accuracy: 0.7440\n",
      "Epoch 5/10\n",
      "13/13 [==============================] - 6s 481ms/step - loss: 0.5249 - accuracy: 0.8034 - val_loss: 0.6179 - val_accuracy: 0.7343\n",
      "Epoch 6/10\n",
      "13/13 [==============================] - 6s 442ms/step - loss: 0.4922 - accuracy: 0.8204 - val_loss: 0.5807 - val_accuracy: 0.7536\n",
      "Epoch 7/10\n",
      "13/13 [==============================] - 5s 389ms/step - loss: 0.4024 - accuracy: 0.8568 - val_loss: 0.5799 - val_accuracy: 0.7633\n",
      "Epoch 8/10\n",
      "13/13 [==============================] - 5s 364ms/step - loss: 0.3601 - accuracy: 0.8665 - val_loss: 0.5454 - val_accuracy: 0.7729\n",
      "Epoch 9/10\n",
      "13/13 [==============================] - 5s 358ms/step - loss: 0.3404 - accuracy: 0.8677 - val_loss: 0.5030 - val_accuracy: 0.8019\n",
      "Epoch 10/10\n",
      "13/13 [==============================] - 5s 363ms/step - loss: 0.3091 - accuracy: 0.8968 - val_loss: 0.5534 - val_accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "# dataset name, IMDB movie reviews dataset\n",
    "dataset_name = \"imdb\"\n",
    "# get the unique model name based on hyper parameters on parameters.py\n",
    "model_name = get_model_name(dataset_name)\n",
    "# load the data\n",
    "data = load_data(N_WORDS, SEQUENCE_LENGTH, TEST_SIZE, oov_token=OOV_TOKEN)\n",
    "# construct the model\n",
    "model = create_model(data[\"tokenizer\"].word_index, units=UNITS, n_layers=N_LAYERS, \n",
    "                    cell=RNN_CELL, bidirectional=IS_BIDIRECTIONAL, embedding_size=EMBEDDING_SIZE, \n",
    "                    sequence_length=SEQUENCE_LENGTH, dropout=DROPOUT, \n",
    "                    loss=LOSS, optimizer=OPTIMIZER, output_length=data[\"y_train\"][0].shape[0])\n",
    "model.summary()\n",
    "# using tensorboard on 'logs' folder\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "# start training\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[tensorboard],\n",
    "                    verbose=1)\n",
    "# save the resulting model into 'results' folder\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(text):\n",
    "    sequence = data[\"tokenizer\"].texts_to_sequences([text])\n",
    "    # pad the sequences\n",
    "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
    "    # get the prediction\n",
    "    prediction = model.predict(sequence)[0]\n",
    "    return prediction, data[\"int2label\"][np.argmax(prediction)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output vector: [0.08682992 0.6174334  0.24125782 0.05447892]\n",
      "Prediction: 2\n"
     ]
    }
   ],
   "source": [
    "text = new_text\n",
    "output_vector, prediction = get_predictions(text)\n",
    "print(\"Output vector:\", output_vector)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = str(data['X_test'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ankitasingh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.4.2.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.graph_objs as go\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from plotly.offline import iplot\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/ankitasingh/Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Downloads/ECS192/Datasets/Replication_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Replication</th>\n",
       "      <th>Score_replication</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it will no influence DNA replication</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The G to A base change will not affect replica...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>During DNA replication the entire DNA will be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In DNA replication the DNA would simply replic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It wont have any direct effect besides that th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>It will stop the function of the DNA.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>It will stop replication early.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>It will stop earlier then it should.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>The DNA will stop replication at the stop codo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>It will likely not proceed.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1031 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Replication  Score_replication\n",
       "0                  it will no influence DNA replication                  1\n",
       "1     The G to A base change will not affect replica...                  1\n",
       "2     During DNA replication the entire DNA will be ...                  1\n",
       "3     In DNA replication the DNA would simply replic...                  1\n",
       "4     It wont have any direct effect besides that th...                  1\n",
       "...                                                 ...                ...\n",
       "1026              It will stop the function of the DNA.                  3\n",
       "1027                    It will stop replication early.                  3\n",
       "1028               It will stop earlier then it should.                  3\n",
       "1029  The DNA will stop replication at the stop codo...                  3\n",
       "1030                        It will likely not proceed.                  3\n",
       "\n",
       "[1031 rows x 2 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Replication'].tolist()\n",
    "y = df['Score_replication'].tolist()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['Replication'] = df['Replication'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Replication'] = df['Replication'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1038 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['Replication'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (1031, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Replication'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (1031, 3)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['Score_replication']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(824, 250) (824, 3)\n",
      "(207, 250) (207, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 250, 100)          5000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 5,080,703\n",
      "Trainable params: 5,080,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 5s 392ms/step - loss: 0.4665 - accuracy: 0.8240 - val_loss: 0.6249 - val_accuracy: 0.7515\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 4s 384ms/step - loss: 0.3899 - accuracy: 0.8604 - val_loss: 0.6416 - val_accuracy: 0.7758\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 4s 379ms/step - loss: 0.3079 - accuracy: 0.9044 - val_loss: 0.5773 - val_accuracy: 0.8000\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 5s 424ms/step - loss: 0.2857 - accuracy: 0.9120 - val_loss: 0.6155 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 4s 368ms/step - loss: 0.2074 - accuracy: 0.9317 - val_loss: 0.6219 - val_accuracy: 0.8061\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 4s 362ms/step - loss: 0.2679 - accuracy: 0.9226 - val_loss: 0.5942 - val_accuracy: 0.8061\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.2,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 40ms/step - loss: 0.7331 - accuracy: 0.7778\n",
      "Test set\n",
      "  Loss: 0.733\n",
      "  Accuracy: 0.778\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.893483   0.02102285 0.08549421]] 1\n"
     ]
    }
   ],
   "source": [
    "new_complaint = ['DNA replication will not be affected.']\n",
    "seq = tokenizer.texts_to_sequences(new_complaint)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = ['1', '2', '3']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Classification with Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Replication_scored_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0, 2: 1, 3: 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.Score_replication.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.Score_replication.replace(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Replication</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Score_replication</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>train</th>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>train</th>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>train</th>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Replication\n",
       "Score_replication label data_type             \n",
       "1                 0     train              426\n",
       "                        val                 76\n",
       "2                 1     train              166\n",
       "                        val                 29\n",
       "3                 2     train              284\n",
       "                        val                 50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['Score_replication', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].Replication.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt',\n",
    "    truncation= True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].Replication.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "    return_tensors='pt',\n",
    "    truncation = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)\n",
    "\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d8f9aa14df4cb1ac6c9c7f4cd469ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00463116e8e41ed94961e9fa11b04b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch 1'), FloatProgress(value=0.0, max=176.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9306225bb55a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss_train_total\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 1\n",
      "Accuracy: 63/76\n",
      "\n",
      "Class: 2\n",
      "Accuracy: 12/29\n",
      "\n",
      "Class: 3\n",
      "Accuracy: 44/50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('finetuned_BERT_epoch_5.model', map_location=torch.device('cpu')))\n",
    "\n",
    "_, predictions, true_vals = evaluate(dataloader_validation)\n",
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Bert approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries needed\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Replication_scored_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.Replication[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.Score_replication[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1031, 2)\n",
      "TRAIN Dataset: (825, 2)\n",
      "TEST Dataset: (206, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 4)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistillBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the loss function and optimizer\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calcuate the accuracy of the model\n",
    "\n",
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the training function on the 80% of the dataset for tuning the distilbert model\n",
    "\n",
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples \n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.5235838890075684\n",
      "Training Accuracy per 5000 steps: 25.0\n",
      "The Total Accuracy for Epoch 0: 54.303030303030305\n",
      "Training Loss Epoch: 1.0258348341581327\n",
      "Training Accuracy Epoch: 54.303030303030305\n",
      "Training Loss per 5000 steps: 0.31326359510421753\n",
      "Training Accuracy per 5000 steps: 100.0\n",
      "The Total Accuracy for Epoch 1: 74.54545454545455\n",
      "Training Loss Epoch: 0.7192864210709281\n",
      "Training Accuracy Epoch: 74.54545454545455\n",
      "Training Loss per 5000 steps: 1.12823486328125\n",
      "Training Accuracy per 5000 steps: 25.0\n",
      "The Total Accuracy for Epoch 2: 80.96969696969697\n",
      "Training Loss Epoch: 0.530789656051691\n",
      "Training Accuracy Epoch: 80.96969696969697\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if _%5000==0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the validation section to print the accuracy and see how it performs\n",
      "Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch\n",
      "Validation Loss per 100 steps: 0.9930866360664368\n",
      "Validation Accuracy per 100 steps: 50.0\n",
      "Validation Loss Epoch: 0.639574985481003\n",
      "Validation Accuracy Epoch: 72.81553398058253\n",
      "Accuracy on test data = 72.82%\n"
     ]
    }
   ],
   "source": [
    "print('This is the validation section to print the accuracy and see how it performs')\n",
    "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
    "\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved\n",
      "This tutorial is completed\n"
     ]
    }
   ],
   "source": [
    "# Saving the files for re-use\n",
    "\n",
    "output_model_file = '/models/pytorch_distilbert_news.bin'\n",
    "output_vocab_file = '/models/vocab_distilbert_news.bin'\n",
    "\n",
    "model_to_save = model\n",
    "torch.save(model_to_save.state_dict, 'bert_model.pt')\n",
    "tokenizer.save_vocabulary('vocab_distilbert_news.bin')\n",
    "\n",
    "print('All files saved')\n",
    "print('This tutorial is completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roberta Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Replication_scored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Replication': 'text', 'Score_replication': 'labels'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "df['labels'] = df['labels'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('roberta', 'roberta-base', num_labels=4, use_cuda=False, \n",
    "                            args={'learning_rate':1e-5, 'num_train_epochs': 3,\n",
    "                                  'reprocess_input_data': True, 'overwrite_output_dir': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4988de683bbf4a27b5242cea20e6e513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=824.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdc8361ee264daea14cfb8ad8af6163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffa27ebb8f884b02887103b63e29a698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 3'), FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1276e7681dfd47aa8fd67d7e53cb5094",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 1 of 3'), FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa8657aa77348cd858481409078c758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 2 of 3'), FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(309, 0.8735930786067229)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f73616122045ea8057d5cff788db44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=207.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b706a1a1bb404e825c071712c4fa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Evaluation'), FloatProgress(value=0.0, max=26.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "    \n",
    "result, model_outputs, wrong_predictions = model.eval_model(test, f1=f1_multiclass, acc=accuracy_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mcc': 0.763392415762375,\n",
       " 'f1': 0.855072463768116,\n",
       " 'acc': 0.855072463768116,\n",
       " 'eval_loss': 0.4600811147919068}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with XLNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Replication_scored.csv')\n",
    "df.rename(columns={'Replication': 'text', 'Score_replication': 'labels'}, inplace = True)\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['labels'] = df['labels'].astype(int)\n",
    "train, test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'sequence_summary.summary.weight', 'logits_proj.weight', 'sequence_summary.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model = ClassificationModel('xlnet', 'xlnet-base-cased', num_labels=3, use_cuda=False, \n",
    "                            args={'learning_rate':1e-5, 'num_train_epochs': 2,\n",
    "                                  'reprocess_input_data': True, 'overwrite_output_dir': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a294f00c7274020bad2a877d4a98a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=824.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33308fc190147c7a0664c1df7bc883b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731a236c6e3844c7aa96c65f8805f993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 2'), FloatProgress(value=0.0, max=103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 3 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-714ec37ce099>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, train_df, multi_label, output_dir, show_running_loss, args, eval_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m         global_step, training_details = self.train(\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, output_dir, multi_label, show_running_loss, eval_df, test_df, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    886\u001b[0m                         )\n\u001b[1;32m    887\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m                     loss, *_ = self._calculate_loss(\n\u001b[0m\u001b[1;32m    889\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/simpletransformers/classification/classification_model.py\u001b[0m in \u001b[0;36m_calculate_loss\u001b[0;34m(self, model, inputs, loss_fct, num_labels, args)\u001b[0m\n\u001b[1;32m   2256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_calculate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2258\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2259\u001b[0m         \u001b[0;31m# model outputs are always tuple in pytorch-transformers (see doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/transformers/models/xlnet/modeling_xlnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, mems, perm_mask, target_mapping, token_type_ids, input_mask, head_mask, inputs_embeds, labels, use_mems, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1569\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1570\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1571\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m    932\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2317\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 3 is out of bounds."
     ]
    }
   ],
   "source": [
    "model.train_model(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "def f1_multiclass(labels, preds):\n",
    "    return f1_score(labels, preds, average='micro')\n",
    "    \n",
    "result, model_outputs, wrong_predictions = model.eval_model(test, f1=f1_multiclass, acc=accuracy_score, conf=confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result['conf']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
